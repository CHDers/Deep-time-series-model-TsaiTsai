{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecd281d4-e3df-4e8e-aa98-45743e3b8c1b",
   "metadata": {},
   "source": [
    "# **深度学习公开课 - 深度学习中的时间序列算法群**\n",
    "> 节选自《深度学习实战》第7期正课<br>\n",
    "> 作者：@菜菜TsaiTsai<br>\n",
    "> 版本号：2023/10/18<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fd7f9a-dab5-4f07-b9e6-bb45de3175fc",
   "metadata": {},
   "source": [
    "## **<center><font color =\"k\">公开课Day1 LSTM与深度学习中的时间序列<br><br>极致易学 | 高效入门 | 前景讨论<center>**<br>**<center><font color =\"red\">直播将于8点05分正式开始！<br><br>扫码回复\"DL999\"领取今日直播课件>>><br><br>扫码回复\"优惠\"抢直播间专属优惠券>>></font></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1b52da-1768-434b-b3b4-913c18794a66",
   "metadata": {},
   "source": [
    "## 0 课程规划\n",
    "\n",
    "欢迎来到《深度学习中的时间序列算法群》公开课。在这门课程中，我将带你从0认识3大类深度学习中的时间序列模型，并为你深度讲解深度时序算法众多的的精彩理念与实现方式。当你完成这门课程时，你将完成深度时序算法入门，打好进一步学习更多高级架构的基础。\n",
    "\n",
    "**DAY 1：LSTM与深度学习中的时间序列**\n",
    "1. 深度学习中的时间序列数据\n",
    "2. 时序数据 vs 非时序数据\n",
    "3. 循环神经网络如何处理时序问题\n",
    "4. LSTM的灵感起源与直觉理解\n",
    "5. LSTM的基本结构与架构设计\n",
    "\n",
    "**DAY 2：LSTM的参数全解与预测实战**\n",
    "1. PyTorch中的LSTM层与参数\n",
    "2. LSTM在时序数据上的预测实战\n",
    "3. 深度学习中的时间序列解决方案\n",
    "……\n",
    "\n",
    "更多后续课程请关注B站动态和小可爱私聊信息！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973ece9b-bc1c-4568-a438-8c615f689a55",
   "metadata": {},
   "source": [
    "## 1 学前自测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f5c21-ec92-4a75-b1e7-9928ef05763b",
   "metadata": {},
   "source": [
    "-【Q1】你了解深度学习中的时间序列吗？比如，时间序列一般是几维的数据？每个维度的名字是什么，通常代表了什么含义？\n",
    "\n",
    "-【Q2】为什么普通的机器学习/深度学习算法无法处理时间序列数据？你了解时序算法设计过程中的核心需求吗？\n",
    "\n",
    "-【Q3】你知道循环类神经网络，尤其是RNN处理时间序列数据的思路吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a549d1a-d171-41cd-bd8f-58397640c300",
   "metadata": {},
   "source": [
    "### 1.1 深度学习中的时间序列数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab88df6-4572-4eb3-a1b3-f89dd3ad47b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "【Q1】你了解深度学习中的时间序列吗？比如，时间序列一般是几维的数据？每个维度的名字是什么，通常代表了什么含义？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9076aabf-2715-49d2-aa04-bbf3eef45407",
   "metadata": {
    "tags": []
   },
   "source": [
    "> 机器学习中典型的时间序列数据\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/02.png)\n",
    "\n",
    "通常来说是一维的序列或者二维表单。在时间序列数据中，每个样本是一个时间点，样本与样本之间按照时间顺序排列。**在深度学习的世界中，时间的维度被称之为是时间步（time_step）维度**，一个时间点是一个时间步，时间步的总数代表了当前时间序列的长度，因此这一维度也被称为序列长度（sequence_length）。\n",
    "\n",
    "在一维序列中，唯一的信息就是时间步与当前时间步对应的标签。在二维序列中，第二维度input_dimension代表的是当前时间步上的特征信息。\n",
    "\n",
    "> 深度学习中典型的时间序列数据\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/03_.png)\n",
    "\n",
    "在深度学习中，时间序列通常是三维或更高维的表单，因为深度学习算法会**同时处理多个内在逻辑相同的时间序列**。其中time_step和input_dimension决定了一个时间序列的序列长度和特征量，而batch_size决定了整个数据集中一共有多少个二维时间序列表单。这些二维表单堆叠在一起，构成深度学习算法输入所必备的三维时间序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5116e3ac-9828-4f70-b352-d4b212caa50a",
   "metadata": {
    "tags": []
   },
   "source": [
    "其实这样的三维时间序列也可以用二维方式来表示。你或许已经发现了，其实三维时间序列数据就是机器学习中定义的“多变量时间序列数据”。在多变量时间序列数据当中，时间和另一个因素共同决定唯一的特征值。在上面的例子中，每张时序二维表代表一支股票，batch_size决定了一共有多少支股票，因此在这个多变量时间序列数据中“时间”和“股票编号”共同决定了一个时间点上的值。如果在机器学习中，我们会看到这样的数据结构：\n",
    "\n",
    "|股票ID|时间|开盘价|收盘价|交易量|最高价|最低价|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|00K621|6月1日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00K621|6月2日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00K621|6月3日|xxx|xxx|xxx|xxx|xxx|\n",
    "|……|||||||\n",
    "|00E504|6月1日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00E504|6月2日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00E504|6月3日|xxx|xxx|xxx|xxx|xxx|\n",
    "|……|||||||\n",
    "|00H829|6月1日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00H829|6月2日|xxx|xxx|xxx|xxx|xxx|\n",
    "|00H829|6月3日|xxx|xxx|xxx|xxx|xxx|\n",
    "|……|||||||\n",
    "\n",
    "当把这张表单拆成独立的三张表单，每张表单上只显示一支股票时，就是深度学习中常见的三维时间序列数据。相似的例子还可能是——不同用户在不同时间点上的行为，不同植物在不同季节时分泌的激素值、不同商家在不同时间点上的销售额等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf43fb4-6969-4e90-a846-768673781fdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "在现实的生产生活中，时间序列数据几乎无处不在——例如，金融市场的股票价格预测、用户违约行为预测；医疗领域的心电图预测、病程发展预测；环境科学中的天气的预测、灾害预警；能源领域中的供电需求预测、天然气或太阳能的能源输出量预测等等……**只要是依赖于使用历史信息预测未来的场景，都会涉及到时间序列数据的预测**，甚至，只要给原始的非序列数据加上一个“时间顺序”或者“位置顺序”，就能够立刻让非序列数据化身时间序列数据，这些时序数据的维度可以是四维、五位、甚至更高维。所以在深度学习中，文本数据、语音数据、视频数据，都一定程度上被囊括在时间序列的范围内，毕竟——\n",
    "\n",
    "> - 文字数据是按照时间顺序排列的一个个字/词（三维）\n",
    "> - 语音数据是按照时间顺序排列的一个个音频信号（四维）\n",
    "> - 视频数据是按照时间顺序来排列的一张张图片（五维）\n",
    "\n",
    "毫不夸张的说，虽然研究序列数据和序列算法的领域是NLP（自然语言处理），但几乎所有自然语言处理架构下的算法都与时间序列数据的处理和预测相关。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d17ad5-1b28-4dd5-9087-796377ada4e8",
   "metadata": {},
   "source": [
    "### 1.2 时序数据 vs 非时序数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c68727-0552-403e-bd17-895b0674bdc1",
   "metadata": {
    "tags": []
   },
   "source": [
    "【Q2】为什么普通的机器学习/深度学习算法无法处理时间序列数据？你了解时序算法设计过程中的核心需求吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba9a7e8-0efa-486c-9691-52d6cd306052",
   "metadata": {
    "tags": []
   },
   "source": [
    "在大多数机器学习和深度学习的预测过程中，我们使用的都是表格、图像等非序列数据，这些数据的样本与样本之间相互独立、每个样本及其特征对应了唯一的标签，就像如下的表单：\n",
    "\n",
    "![01](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/01.png)\n",
    "\n",
    "在非序列数据上，在训练时我们可以随意交换样本顺序，假设有少数样本有缺失也无关紧要，在建模时我们探索的是**一个样本的特征与其标签之间**的关联，因此无论我们先训练1号样本、还是先训练7号样本、还是只训练数据集中的一部分样本，都不会从本质上改变数据的含义、许多时候也不会改变算法对数据的理解和学习结果。\n",
    "\n",
    "然而对序列数据来说，情况则大不相同。**序列数据是样本与样本之间有着特定的逻辑联系的数据，序列数据通常按照样本与样本之间的逻辑顺序排列，且这种顺序不能被轻易修改和打乱的数据**，序列数据包括但不限于文字序列、时间序列、语音序列、视频序列、DNA序列等等典型的序列数据。\n",
    "\n",
    "对序列数据来说，一旦**调换样本顺序**或**样本发生缺失**，就可能对数据的含义有重大的影响（例如：事倍功半和事半功倍）。**在序列数据上，我们不仅要学习特征与标签之间的关联，还要学习样本与样本之间的关联**，因为序列数据中的样本会根据排列顺序影响彼此，并最终影响到标签的输出和算法的结果。考虑下面的时间序列数据类型：\n",
    "\n",
    "> - 文字数据是按照时间顺序排列的一个个字/词（三维）\n",
    "> - 语音数据是按照时间顺序排列的一个个音频信号（四维）\n",
    "> - 视频数据是按照时间顺序来排列的一张张图片（五维）\n",
    "\n",
    "我们必须要学习文字与文字之间的关联，才能够理解语义。必须要将语音和语音信号关联起来，才能够理解旋律或段落的信息……任何序列数据上，我们都需要学习样本与样本之间的关系，而普通的机器学习/深度学习算法不具备这样的能力，因此我们需要单独设计算法来解决这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586259db-4d15-49d4-8023-305309630e93",
   "metadata": {},
   "source": [
    "### 1.3 循环神经网络RNN如何处理序列问题？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a562f8-45ec-4e3f-970e-18cdf69953f3",
   "metadata": {},
   "source": [
    "【Q3】你知道循环类神经网络，尤其是RNN处理时间序列数据的思路吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d583ff-8108-4157-afc1-bd3667260662",
   "metadata": {},
   "source": [
    "在众多时间序列的解决方案中，循环神经网络家族是最经典的解决方案之一，它处理序列数据的基本思路是：**按顺序依次处理每个时间步上的信息，并将上一个时间步的信息传递给下一个时间步，以此建立时间步与时间步之间的联系**。\n",
    "\n",
    "首先，循环神经网络的网络结构与普通神经网络一致，它是由多个线性层构成的神经网络，它有基本的输入、隐藏和输出层，单从网络架构来看与深度神经网络几乎别无二致。在处理序列数据时，**循环神经网络会一个时间步、一个时间步地进行处理**，从表单的角度来看，就是在每张表单上一个样本、一个样本地进行处理。故而，在循环神经网络的一次正向传播中，网络只会接触到一个单词/一个时间步的信息，而不会接触到整张表。很显然，如果一个循环神经网络要看完整个句子，就会需要sequence_length次正向传播。时间序列越长，需要的正向传播次数就越多。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79059d05-df5c-4725-bcc7-827efc1fb42f",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/11.png)\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/12.png)\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ded96bd-6e5b-4374-95ae-038cf8454df3",
   "metadata": {},
   "source": [
    "在此基础上，循环神经网络如何让上一个时间步的信息传递到下一个时间步呢？**循环网络在不同时间步的隐藏层之间建立了链接**，如下图所示：\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310b3157-5b81-4af0-aef3-d4389bbb9769",
   "metadata": {},
   "source": [
    "假设当前时间步是t-1，当前时间步上的输入特征为$X_{t-1}$，输入层与隐藏层之间的的权重为$W_{xh}$，隐藏层与输出层之间的权重为$w_{hy}$，当$X_{t-1}$进入神经网络后时，权重$W_{xh}$将与输入信息$X_{t-1}$共同计算，构成中间变量$H_{t-1}$，这一中间变量被称之为是“隐藏状态”，代表在隐藏层上输出的值。\n",
    "\n",
    "在深度神经网络中，$H_{t-1}$将会被传导向输出层，与$w_{hy}$共同计算后构成输出层上的输出，但在循环神经网络中，$H_{t-1}$除了被传导向输出层之外，还会被传导向下一个时间步，与$X_{t}$一起，共同构建$H_{t}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031af13-30dd-4af1-9797-8b44a0666e39",
   "metadata": {},
   "source": [
    "具体地来看：\n",
    "\n",
    "- 普通神经网络\n",
    "\n",
    "$$H = f(W_{xh}X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daecdc94-9b25-4610-a852-5b468b3e6a41",
   "metadata": {},
   "source": [
    "其中f是激活函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b4d247-5870-4553-9ed2-055528b9d36a",
   "metadata": {},
   "source": [
    "- 循环神经网络：\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "H_t &= f(\\color{red}{W_{hh}H_{t-1}} + W_{xh}X_t) \\\\ \\\\ \n",
    "&= f(W_{hh}f(\\color{red}{W_{hh}H_{t-2}} + W_{xh}X_{t-1}) + W_{xh}X_t) \\\\ \\\\\n",
    "&= f(W_{hh}f(W_{hh}f(\\color{red}{W_{hh}H_{t-3}} + W_{xh}X_{t-2}) + W_{xh}X_{t-1}) + W_{xh}X_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "其中，$W_{hh}$是循环网络中，隐藏层与隐藏层之间链接上的权重。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/15.png)\n",
    "\n",
    "利用这种方式，只要进行sequence_length次向前传播，并且每次都将上一个时间步中隐藏层上诞生的中间变量传递给下一个时间步的隐藏层，整个网络就能在全部的正向传播完成后获得整个句子上的全部信息。在这个过程中，我们在同一个网络上不断运行正向传播，**此过程在神经网络结构上是循环，在数学逻辑上是递归，这也是循环神经网络名称的由来**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2ba1d7-584b-4a1a-ad83-5540dddd127d",
   "metadata": {},
   "source": [
    "这种传递方式可以让循环神经网络“记得”历史时间步上的信息，理论上来说，在最后一个时间步上输出的变量H_T应该包含从t=0到t=T的所有时间步上的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f594202-b6d3-4f0f-a1e3-2088bb5299f6",
   "metadata": {},
   "source": [
    "好了，到这里我们铺设好了学习LSTM所必须的全部基础知识。现在让我们来看一下LSTM。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49f6163-f62d-48f5-b926-dfa455784968",
   "metadata": {},
   "source": [
    "## 2 LSTM基本架构与原理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4172a45-8430-4bcb-8b01-ae37b5a1cccf",
   "metadata": {},
   "source": [
    "### 2.1 LSTM的灵感起源与直觉理解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49496f5a-a95a-4da0-b40b-17f24b4f1e4c",
   "metadata": {},
   "source": [
    "长短期记忆网络（Long Short-Term Memory Network，简称LSTM）是一种特殊的循环神经网络，它于1997年被慕尼黑技术大学的两位神经网络研究者提出，是当代深度学习领域中对时序数据和文字数据都具有重要作用的核心架构之一。在NLP的世界中，RNN为神经网络赋予了“记忆”的能力，LSTM则实现了对这种“记忆”方式的改良和优化。\n",
    "\n",
    "首先，从**直觉**上来看，RNN的“记忆机制”存在两大关键问题：\n",
    "\n",
    "- **所有信息照单全收，毫无重点**\n",
    "\n",
    "在RNN的机制中，每处理一个时间步，就要在网络上进行一次正向传播，而每次正向传播都要使用到神经元之间的权重矩阵$W$。为了节约计算资源、降低运算复杂度，所有时间步在进行正向传播时所使用的权重矩阵$W$都是相同的。只有当全部的时间步都经过正向传播后，RNN才会统一对权重矩阵$W$进行一次迭代，新的权重矩阵$W^*$又会被使用到全部的时间步上再次进行正向传播。在这个流程中，一个序列中的多个时间步共享权重，所有的时间步都被相同的权重进行处理，因此RNN处理所有时间步信息的方式是完全相同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803698c2-74d0-49cb-b2de-0b7f75387dc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20632c25-3912-4521-8c34-48fa455b4c88",
   "metadata": {},
   "source": [
    "但是从常识来判断，无论是在时间序列中还是在文字序列中，过去时间步上的信息对未来时间步上信息的影响可能是不同的。举个很简单的例子——\n",
    "\n",
    "居民区每日用电量有周期性，夜晚用电量会增加，白日用电量较少。在这种情况下，如果我们要预测每晚23点时的用电量，那过去夜里的时间步上的信息就会极为重要，而白日的时间步上的信息就没有太大帮助。\n",
    "\n",
    "但是对于所有这些信息，RNN都使用同样的权重来进行处理，因此在吸纳信息时，RNN**无法辨别不同时间点上的信息对未来的不同影响**，在使用这些信息进行预测的时候，RNN**也无法辨别不同时间步上的标签最需要的信息是哪些**，这会严重影响算法的整体预测效果。到今天，这依然是影响RNN预测效果的关键因素之一。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd57421-9517-4827-ac10-e073dc85df33",
   "metadata": {},
   "source": [
    "- **新信息强制覆盖旧信息，导致遗忘**\n",
    "\n",
    "在RNN中，隐藏状态$H_t$在每个时间步都会进行更新：\n",
    "\n",
    "$$H_t = f(\\color{red}{W_{hh}H_{t-1}} + W_{xh}X_t)$$\n",
    "\n",
    "在更新过程中，不断有新的$X_t$被加入到隐藏状态当中，这一加法过程就会自然地挤压历史信息$H_{t-1}$在新的隐藏状态$H_t$中所占的信息比重。在RNN中，激活函数f通常是tanh函数或sigmoid函数，这意味着隐藏状态$H_t$的大小大概率会被限制在一个固定的区间之内。当$H_t$大小被限制在[0,1]或者[-1,1]范围内时，随着新信息的增多，历史信息的空间就会被压缩得越来越厉害。当时间步的数量太多、序列太长的时候，历史信息所占的比重就会被压缩到一定程度，此时许多历史信息对$H_t$的影响就可以忽略不计，等同于RNN将这部分历史信息给遗忘了。\n",
    "\n",
    "在RNN的过程中，这种遗忘是强制的、并且很难被改善。因为根据RNN的数据流过程，$H_t$计算过程中必然存在大量的嵌套和信息挤压现象。事实上，虽然理论上最后一个时间步的$H_T$中包含了所有时间步上的信息，但是真正有影响力的只要非常接近最后一个时间步的几个时间步而已。大部分信息都被RNN遗忘，导致RNN很难处理较长的序列。\n",
    "\n",
    "基于以上两点问题，研究者们在设计LSTM的时候存在一个根本诉求——<font color=\"red\">**要创造一个全新的架构、一套全新的数据流，为循环神经网络赋予选择性记忆和选择性传递信息的能力**</font>。这里的选择性包含多层含义，包括：\n",
    "\n",
    "**1. 循环网络必须自行选择吸纳多少新信息**，只留重点，拒绝照单全收\n",
    "\n",
    "**2. 循环网络必须自行选择遗忘多少历史信息**，主动遗忘无效内容，保留有效内容\n",
    "\n",
    "**3. 循环网络必须自行判断、对当前时间步的预测来说最重要的信息是哪些，并将该信息输出给当前时间步**，这样既可以保证当前时间步上的预测是最高效的，也不会影响向下一个时间步传递的信息。\n",
    "\n",
    "在这三个能力当中，前两个能力允许循环神经网络决定“哪些长期信息会被传递下去”，最后一个能力允许循环神经网络决定“哪些短期信息对当前时间步的预测是最重要的”。这三种能力构成了LSTM的基本结构。接下来就让我们来看看LSTM是如何实现以上三种能力的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90cc17-8b95-4f9b-a648-1adb41b0f754",
   "metadata": {},
   "source": [
    "### 2.2 LSTM的基本结构与架构设计"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58104c4a-5a50-4ecd-85e9-eebb63b24730",
   "metadata": {},
   "source": [
    "LSTM算法的结构本身十分繁复，无论是从网络架构还是数学原理角度出发，我们都很难使用简单的语言将LSTM完整呈现。然而幸运的是，在了解研究者们要为RNN增加的三大类能力后，我们会发现LSTM的思路是非常清晰的。让我们一起来看看LSTM的基本结构——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d905ae6-9d87-4e1c-aaf7-33a7824e7da2",
   "metadata": {},
   "source": [
    "- 记忆细胞"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cb8c8c-cc1a-407e-99ed-d5974bf82326",
   "metadata": {},
   "source": [
    "首先，**LSTM依然是一个循环神经网络，因此LSTM的网络架构与RNN高度相似，同时LSTM也是需要遍历所有时间步，不断完成循环和嵌套的**。但不同的是，RNN由输入层（输入$X_t$）、隐藏层和输出层（输出$Y_t$）构成，而LSTM由输入层（输入$X_t$）、**记忆细胞（Memory Cell）**和输出层（输出$Y_t$）构成，其中输入、输出层与RNN的输入、输出层完全一致，而记忆细胞是LSTM独有的结构。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/02_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e138d7-19b7-421b-8adc-f80a80e40f6c",
   "metadata": {},
   "source": [
    "**记忆细胞是LSTM的基本计算单元，在记忆细胞中，我们分割长期信息与短期信息，同时赋予循环网络对信息做选择的能力**。在之前我们提到，循环网络必须自行决定哪些长期信息会被传递下去，哪些短期信息对当前的预测最为有效，因此在记忆细胞当中，LSTM设置了两个关键变量：\n",
    "\n",
    "> - **主要负责记忆短期信息、尤其是当前时间步信息的隐藏状态$h$，**以及\n",
    "> - **主要负责长期记忆的细胞状态$C$**\n",
    "\n",
    "这两个变量都随着时间步进行迭代。如上图所示，在迭代开始时，LSTM会同时初始化$h_0$和$C_0$；在任意时间步t上，记忆细胞会同时接受到来自上一个时间步的长期记忆$C_{t-1}$、短期信息$h_{t-1}$以及当前时间步上输入的新信息$X_t$三个变量，结合三者进行运算后，记忆细胞会输出当前时间步上的长期记忆$C_{t}$和短期信息$h_{t}$，并将它们传递到下一个时间步上。同时，在每个时间步上，$h_t$还会被传向当前时间步的输出层，用于计算当前时间步的预测值$\\hat{y}_t$。\n",
    "\n",
    "那在记忆细胞的内部，究竟是如何进行具体的$C_t$和$h_t$计算的呢？让我们来看看记忆细胞内部的流程图：\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/LSTM%E6%B5%81%E7%A8%8B%E5%9B%BE1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928a886a-989c-4103-8c25-e08d56f196db",
   "metadata": {},
   "source": [
    "这是一个复杂的流程，但在横向上，它可以被分割为C的传递和h的传递两条路径；在纵向上，它可以被分为如图所示的三个不同的路径：\n",
    "\n",
    "**1. 帮助循环网络选择吸纳多少新信息的输入门**\n",
    "\n",
    "**2. 帮助循环网络选择遗忘多少历史信息的遗忘门**，以及\n",
    "\n",
    "**3. 帮助循环网络选择出对当前时间步的预测来说最重要的信息、并将该信息输出给当前时间步的输出门**\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/LSTM%E6%B5%81%E7%A8%8B%E5%9B%BE2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874d13a0-e2e5-423c-8a38-601108321e38",
   "metadata": {},
   "source": [
    "让我们分别来看一下三个门是如何工作的。\n",
    "\n",
    "- **遗忘门**\n",
    "\n",
    "**遗忘门是决定要留下多少长期信息C的关键计算单元，其数学本质是为上一个时间步传入的$C_{t-1}$赋予一个[0,1]之间的权重，以此筛选掉部分旧信息。**在这个计算过程中，假设遗忘门赋予$C_{t-1}$的权重为0.7，那就是说遗忘门决定了要保留70%的历史信息，遗忘30%的历史信息，这30%的信息空间就可以留给全新的信息来使用。\n",
    "\n",
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/09.png)\n",
    "\n",
    "那权重是如何被计算出来的呢？如图所示，遗忘门会参考当前时间步的信息$X_t$与上一个时间步的短时信息$h_{t-1}$来计算该权重，其中$\\sigma$是sigmoid函数，$w_f$是动态影响最终权重大小的参数，$f_t$就是[0,1]之间的、用于影响$C_{t-1}$的权重。\n",
    "\n",
    "P.S. 注意，在以往的课程中，我们会将$w$称之为是权重，在LSTM中为区分我们将其称为算法的参数。\n",
    "\n",
    "在LSTM的设计逻辑之中，考虑$X_t$和$h_{t-1}$实际是在考虑离当前时间步最近的**上下文信息**，而参数$w_f$会受到**损失函数和算法整体表现的影响，不断调节遗忘门中计算出的权重f的大小**，因此遗忘门能够结合上下文信息、损失函数传来的梯度信息、以及历史信息共同计算出全新的、被留下的长期记忆$C_t$。这个流程在实践中被证明是十分有效的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f0f468-e1e8-4900-b9e4-5f778928caad",
   "metadata": {},
   "source": [
    "- **输入门**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9f520b-b381-4460-b919-78ed60192909",
   "metadata": {},
   "source": [
    "**输入门是决定要吸纳多少新信息来融入长期记忆C的计算单元，其数学本质是为当前时间步传入的所有信息赋予一个[0,1]之间的权重，以筛选掉部分新信息，将剩余的新信息融入长期记忆C**。\n",
    "\n",
    "在这个计算过程中，我们首先要计算出当前时间步总共吸收了多少全新的信息$\\tilde{C}_t$，这个计算全新信息的方式就与RNN中计算$h_t$的方式高度相似，因此也会包含用于影响新信息传递的参数$W_C$和RNN中常见的tanh函数。然后，我们要依据上下文信息（依然是$X_t$和$h_{t-1}$）以及参数$W_i$来生成筛选新信息的权重$i_t$。最后我们将二者相乘，并加入到长期记忆$C$当中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3215a80b-5059-4120-8665-358c3d10687d",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/07.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca597c-4247-40c2-84cd-7bd265716c2b",
   "metadata": {},
   "source": [
    "可以看到，相比起RNN的数据输入过程，LSTM的输入过程灵活了非常多——在输入门当中，我们不仅对输入数据加上了一个传递的权重$i_t$，还分别使用了两个受损失函数影响的参数$W_i$和$W_C$来控制新信息聚合和权重生产的流程。在权重和两大参数的作用下，输入数据可以被高度灵活地调节，以便满足最佳的损失函数需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d949a33-84b4-4399-8207-f3dbf7933741",
   "metadata": {},
   "source": [
    "- **更新细胞状态**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00d1ba3-277f-4611-852d-557b71cfd636",
   "metadata": {},
   "source": [
    "当遗忘门决定了哪些信息要被遗忘，输入门决定了哪些信息要被加入到长期记忆后，就可以更新用于控制长期记忆的细胞状态了。如下图所示，上一个时间步的长期记忆将乘以遗忘门的权重，再加上新信息$\\tilde{C}_t$乘以新信息筛选的权重$i_t$，同时考虑放弃过去的信息、容纳新信息，以此来构成传递给下一个时间步的长期信息$C_t$。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fbe909-3fa4-4ca3-87af-1e62d1d8fdea",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f887933-f361-4fbb-9f55-2a6028e96240",
   "metadata": {},
   "source": [
    "- **输出门**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15246d9-2fa5-46e5-84d2-6a8495d88eb6",
   "metadata": {},
   "source": [
    "最后我们来到了输出门。**输出门是从全新的长期信息$C_t$中筛选出最适合当前时间步的短期信息$h_t$的计算单元，其数学本质是对已经计算好的长期信息$C_t$赋予一个[0,1]之间的权重，以此筛选出对当前时间步最有效的信息用于当前时间步的预测**。具体流程如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cb7705-863e-43e3-9c28-a199dd04de6b",
   "metadata": {},
   "source": [
    "![](http://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/08.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4033706e-389e-4696-88ea-fe69220c0696",
   "metadata": {},
   "source": [
    "这个流程分为三步:\n",
    "1. 首先要借助上下文信息和参数$W_o$来求解出权重$o_t$\n",
    "2. 对长期信息$C_t$进行tanh标准化处理\n",
    "3. 将$o_t$乘在标准化后的长期信息$C_t$之上，用于筛选出$h_t$。\n",
    "\n",
    "为什么要对长期信息$C_t$做标准化处理呢？在LSTM的论文中如此写到：Tanh标准化可以限制有效长期信息$C_t$的数字范围，避免历史信息在传递过程中变得越来越大，同时还能为输出门赋予一定的非线性性质，这个过程被实践证明有助于保持训练稳定、还能够强化算法学习能力，因此在LSTM最终的设计中被保留下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b36222-74b5-4146-9444-2fbee6a86fa6",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140daac4-405e-467b-aa4d-ed64df239db6",
   "metadata": {},
   "source": [
    "这就是记忆细胞的全部数学过程。很显然，在这个过程中我们能够看到LSTM是如何为RNN赋予了各种灵活的能力，从而实现了多个层次上的“信息筛选”。很显然，LSTM通过分割长短期信息、以及设置3大门的方式，彻底改善了循环神经网络在信息筛选方面的困境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59143d39-f1bb-4947-b62d-2474b9df95ec",
   "metadata": {},
   "source": [
    "现在我们已经了解了LSTM的基本结构，但关于LSTM还有无数的谜题等待我们去解决，例如：\n",
    "\n",
    "- 直觉是如何转化成具体的数学流程的？LSTM的数学流程究竟如何诞生？\n",
    "\n",
    "- 有许多资料声称LSTM可以解决梯度爆炸和梯度消失问题，请问上述流程解决了梯度消失和梯度爆炸问题吗？\n",
    "\n",
    "- 限制于RNN必须依次处理时间步的数学流程，RNN的关键缺陷之一是运算缓慢，LSTM的流程比RNN复杂数倍，LSTM是怎么对抗运算慢的问题的？\n",
    "\n",
    "我们将在《深度学习实战》正课中详细剖析这些问题，并将你对LSTM的原理理解提升到更高的层次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5a55e-61e9-46da-a12a-5311f690ac07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
