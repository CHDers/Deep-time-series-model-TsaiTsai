{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a25e7628-8666-4953-b257-d32ddbdd14b9",
   "metadata": {},
   "source": [
    "# **深度学习公开课 - 深度学习中的时间序列算法群**\n",
    "> 节选自《深度学习实战》第7期正课<br>\n",
    "> 作者：@菜菜TsaiTsai<br>\n",
    "> 版本号：2023/11/7<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a6c394-46a2-4078-a4e6-680634110afb",
   "metadata": {},
   "source": [
    "**<center><font color =\"k\" size=6>Day4 Transformer原理精讲<br>极致易学 | 高效入门 | 把握本质 | 前景讨论<center>**<br>**<center><font color =\"red\" size=6>直播将于8点30分正式开始！<br>扫码回复\"DL999\"领取今日直播课件>>>**<br><br>**<center><font color =\"red\" size=6>双11底价特惠惊爆开场！<br>开场福利，仅限今晚！<br>前10名下单可享超额底价特惠！<br>扫码回复\"优惠\"抢前10名下单名额！<br>>>></font></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db3774-17de-486a-9590-786ac0261732",
   "metadata": {},
   "source": [
    "## 0 课程规划\n",
    "\n",
    "欢迎来到《深度学习中的时间序列算法群》公开课。在这门课程中，我将带你从0认识3大类深度学习中的时间序列模型，并为你深度讲解深度时序算法众多的的精彩理念与实现方式。当你完成这门课程时，你将完成深度时序算法入门，打好进一步学习更多高级架构的基础。\n",
    "\n",
    "**DAY 1：LSTM与深度学习中的时间序列**\n",
    "1. 深度学习中的时间序列数据\n",
    "2. 时序数据 vs 非时序数据\n",
    "3. 循环神经网络如何处理时序问题\n",
    "4. LSTM的灵感起源与直觉理解\n",
    "5. LSTM的基本结构与架构设计\n",
    "\n",
    "**DAY 2：LSTM的参数全解与预测实战**\n",
    "1. PyTorch中的LSTM层与参数\n",
    "2. LSTM类的输入与输出\n",
    "3. 股价与时间序列数据预测实战\n",
    "\n",
    "**DAY 3：时序进阶：时序卷积网络TCN**\n",
    "1. 1d卷积操作与时序数据\n",
    "2. 因果卷积 Causal Convolution\n",
    "3. 1d膨胀卷积与感受野扩张\n",
    "4. 时序卷积网络的架构与实现\n",
    "5. 深度学习中的5大类时序解决方案\n",
    "\n",
    "**DAY 4：Transformer与Attention**\n",
    "1. 序列模型的基本思路与根本诉求\n",
    "2. Attention注意力机制的计算流程\n",
    "3. Transformer架构的Encoder结构层\n",
    "4. Transformer架构的Decoder结构层\n",
    "5. Transformer用于时间序列的相关问题\n",
    "\n",
    "**DAY 5：Informer架构解析与时序应用**\n",
    "\n",
    "**DAY 6：深度时序SOTA架构TabNet**\n",
    "\n",
    "更多后续课程请关注B站动态和小可爱私聊信息！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff08a8e2-42b4-42e8-b6e1-290b736c3ce3",
   "metadata": {
    "tags": []
   },
   "source": [
    "**<center><font color =\"red\" size=6>双11底价特惠惊爆开场！<br>开场福利，仅限今晚！<br>前10名下单可享超额底价特惠！<br>扫码回复\"优惠\"抢前10名下单名额！<br>>>></font></center>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261dc01-661d-4941-8b97-ca07258011f3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/%E8%AF%BE%E7%A8%8B%E5%AE%A3%E4%BC%A05.png)\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/%E8%AF%BE%E7%A8%8B%E5%AE%A3%E4%BC%A06.png)\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/%E8%AF%BE%E7%A8%8B%E5%AE%A3%E4%BC%A09.png)\n",
    "\n",
    "|||\n",
    "|:-:|:-:|\n",
    "|![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/%E8%AF%BE%E7%A8%8B%E5%AE%A3%E4%BC%A01.png)|![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/%E8%AF%BE%E7%A8%8B%E5%AE%A3%E4%BC%A07.png)|\n",
    "\n",
    "|||\n",
    "|:-:|:-:|\n",
    "|![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/%E8%AF%BE%E7%A8%8B%E5%AE%A3%E4%BC%A02.png)|![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/LSTM/%E8%AF%BE%E7%A8%8B%E5%AE%A3%E4%BC%A08.png)|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25eabcf5-b511-4161-b035-cf53822d494a",
   "metadata": {},
   "source": [
    "## 前言：序列模型的基本思路与根本诉求"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe181d2-490c-487e-89f3-aa4a7eab6d09",
   "metadata": {},
   "source": [
    "序列数据是一种按照特定顺序排列的数据，它在现实世界中无处不在，例如股票价格的历史记录、语音信号、文本数据、视频数据等等，主要是按照某种特定顺序排列、且该顺序不能轻易被打乱的数据都被称之为是序列数据。序列数据有着“样本与样本有关联”的特点；对时间序列数据而言，每个样本就是一个时间点，因此样本与样本之间的关联就是时间点与时间点之间的关联。对文字数据而言，每个样本就是一个字/一个词，因此样本与样本之间的关联就是字与字之间、词与词之间的语义关联。很显然，要理解一个时间序列的规律、要理解一个完整的句子所表达的含义，就必须要理解样本与样本之间的关系。\n",
    "\n",
    "对于一般表格类数据，我们一般重点研究特征与标签之间的关联，但**在序列数据中，众多的本质规律与底层逻辑都隐藏在其样本与样本之间的关联中**，这让序列数据无法适用于一般的机器学习与深度学习算法。这是我们要创造专门处理序列数据的算法的根本原因。在深度学习与机器学习的世界中，**序列算法的根本诉求是要建立样本与样本之间的关联，并借助这种关联提炼出对序列数据的理解**。唯有找出样本与样本之间的关联、建立起样本与样本之间的根本联系，序列模型才能够对序列数据实现分析、理解和预测。\n",
    "\n",
    "在机器学习和深度学习的世界当中，存在众多经典且有效的序列模型。这些模型通过如下的方式来建立样本与样本之间的关联——\n",
    "\n",
    "- ARIMA家族算法群\n",
    "> 过去影响未来，因此未来的值由过去的值加权求和而成，以此构建样本与样本之间的关联。\n",
    "\n",
    "$$AR模型：y_t = c + w_1 y_{t-1} + w_2 y_{t-2} + \\dots + w_p y_{t-p} + \\varepsilon_t\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91425e20-e6fd-4440-820a-6ff7ecef7b23",
   "metadata": {},
   "source": [
    "- 循环网络家族\n",
    "> 遍历时间点/样本点，将过去的时间上的信息传递存储在中间变量中，传递给下一个时间点，以此构建样本和样本之间的关联。\n",
    "\n",
    "$$RNN模型：h_t = W_{xh}\\cdot X_t + W_{hh}\\cdot h_{t-1}$$\n",
    "\n",
    "$$LSTM模型：\\tilde{C}_t = tanh(W_{xi} \\cdot X_t + W_{hi} \\cdot h_{t-1} + b_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93585a7c-b431-4963-8702-506947df6777",
   "metadata": {},
   "source": [
    "- 卷积网络家族\n",
    "> 使用卷积核扫描时间点/样本点，将上下文信息通过卷积计算整合到一起，以此构建样本和样本之间的关联。如下图所示，蓝绿色方框中携带权重$w$，权重与样本值对应位置元素相乘相加后生成标量，这是一个加权求和过程。\n",
    "\n",
    "![04](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/1dcnn/04.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a5abd9-3453-4320-8710-8485ba9e5bc2",
   "metadata": {},
   "source": [
    "总结众多序列架构的经验，你会发现**成功的序列架构都在使用加权求和的方式来建立样本与样本之间的关联**，通过对不同时间点/不同样本点上的值进行加权求和，可以轻松构建“上下文信息的复合表示”，只要尝试着使用迭代的方式求解对样本进行加权求和的权重，就可以使算法获得对序列数据的理解。加权求和是有效的样本关联建立方式，这在整个序列算法研究领域几乎已经形成了共识。**在序列算法发展过程中，核心的问题已经由“如何建立样本之间的关联”转变为了“如何求解对样本进行加权求和的权重”**。在这个问题上，Transformer给出了序列算法研究领域目前为止最完美的答案之一——**Attention is all you need，最佳权重计算方式是注意力机制**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad11397-ed66-439b-bbda-6359663c1dd9",
   "metadata": {},
   "source": [
    "## 1 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872c4e7c-f1c3-42e0-8425-0611b9c1f6fb",
   "metadata": {},
   "source": [
    "注意力机制是一个帮助算法辨别信息重要性的计算流程，它通过计算样本与样本之间相关性来判断每个样本在序列中的重要程度，并**给这些样本赋予能代表其重要性的权重**。很显然，注意力机制能够为样本赋予权重的属性与序列模型研究领域的追求完美匹配，Transformer正是利用了自注意力机制的这一特点，从而想到利用注意力机制来进行权重的计算。\n",
    "\n",
    "在注意力基质当中，跨序列进行样本相关性计算的是经典的注意力机制（Attention），在一个序列内部对样本进行相关性计算的是自注意力机制（self-attention）。在Transformer架构中我们所使用的是自注意力机制，因此我们将重点围绕自注意力机制来展开讨论。\n",
    "\n",
    "- 首先，**为什么要判断序列中样本的重要性？**\n",
    "\n",
    "对序列数据来说，每个样本对于理解序列所做出的贡献是不相同的，能够帮助我们理解序列数据含义的样本更为重要，而对序列数据的本质逻辑/含义影响不大的样本则不那么重要。以文字数据为例——\n",
    "\n",
    "**<center>尽管今天<font color =\"green\">下了雨</font>，但我因为拿到了<font color =\"red\">梦寐以求的工作offer</font>而感到<font color =\"red\">非常开心和兴奋</font>。</center>**\n",
    "\n",
    "假设模型对句子进行情感分析，很显然整个句子的情感倾向是积极的，在这种情况下，“下了雨”这一部分对于理解整个句子的情感色彩贡献较小，相对来说，“拿到了梦寐以求的工作offer”和“感到非常开心和兴奋”这些部分则是理解句子传达的正面情绪的关键。因此对序列算法来说，如果更多地学习“拿到了梦寐以求的工作offer”和“感到非常开心和兴奋”这些词，就更有可能对整个句子的情感倾向做出正确的理解，就更有可能做出正确的预测。\n",
    "\n",
    "当我们使用注意力机制来分析这样的句子时，注意力机制可能会为“开心”和“兴奋”这样的词分配更高的权重，因为这些词直接关联到句子的情感倾向。**如果我们能够判断出一个序列中哪些样本是重要的、哪些是无关紧要的，就可以引导算法去重点学习更重要的样本，从而提升模型的表现**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a4906a-2812-46ad-b3e7-51cb48dc277c",
   "metadata": {},
   "source": [
    "- 第二，**那样本的重要性是如何定义的？为什么？**\n",
    "\n",
    "自注意力机制通过**计算样本与样本之间的相关性**来判断样本的重要性，在一个序列当中，如果一个样本与其他许多样本都高度相关，则这个样本大概率会对整体的序列有重大的影响。举例说明，看下面的文字——\n",
    "\n",
    "**<center>经理在会议上宣布了重大的公司<font color =\"red\">重组</font>计划，员工们反应各异，但都对未来充满期待。</center>**\n",
    "\n",
    "在这个例子中，“重组”这个词与“公司”、“计划”、“会议”、“宣布”和“未来”等词汇都高度相关。如果我们针对这些词汇进行提问，你会发现——\n",
    "\n",
    "**公司**发生了什么策略变化？<br>\n",
    "**宣布**了什么内容？<br>\n",
    "**计划**是什么？<br>\n",
    "**未来**会发生什么？<br>\n",
    "**会议**上的主要内容是什么？\n",
    "\n",
    "所有这些问题的答案都是**重组**。很明显，重组这个词不仅提示了事件的性质、是整个句子的关键，而且也对其他词语的理解有着重大的影响。这个单词对于理解句子中的事件——公司正在经历重大变革，以及员工们的情绪反应——都至关重要。如果没有“重组”这个词，整个句子的意义将变得模糊和不明确，因为不再清楚“宣布了什么”以及“未来期待”是指什么。因此，“重组”这个词很明显对整个句子的理解有重大影响，而且它也和句子中的其他词语高度相关。\n",
    "\n",
    "这样的规律可以被推广到许多序列数据上，在序列数据中我们认为**与其他样本高度相关的样本，大概率会对序列整体的理解有重大影响。因此样本与样本之间的相关性可以用来衡量一个样本对于序列整体的重要性**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73db0e-9032-4ea5-9b6d-5d1cc19f41d7",
   "metadata": {},
   "source": [
    "- 第三，**样本的重要性（既一个样本与其他样本之间的相关性）具体是如何计算的？**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a88d4-6b6e-4319-8d0a-0016064adc5e",
   "metadata": {},
   "source": [
    "在NLP的世界中，序列数据中的每个样本都会被编码成一个向量，其中文字数据被编码后的结果被称为词向量，时间序列数据则被编码为时序向量。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/06_.png)\n",
    "\n",
    "因此，要计算样本与样本之间的相关性，本质就是计算向量与向量之间的相关性。**向量的相关性可以由两个向量的点积来衡量**。如果两个向量完全相同方向（夹角为0度），它们的点积最大，这表示两个向量完全正相关；如果它们方向完全相反（夹角为180度），点积是一个最大负数，表示两个向量完全负相关；如果它们垂直（夹角为90度或270度），则点积为零，表示这两个向量是不相关的。因此，向量的点积值的绝对值越大，则表示两个向量之间的相关性越强，如果向量的点积值绝对值越接近0，则说明两个向量相关性越弱。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0af808-75f2-41d1-bb0b-804387fbdf0d",
   "metadata": {},
   "source": [
    "向量的点积就是两个向量相乘的过程，设有两个三维向量$\\mathbf{A}$ 和 $\\mathbf{B}$，则向量他们之间的点积可以具体可以表示为："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06c7ae5-7f41-41d1-b593-eb001c49dcf7",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A} \\cdot \\mathbf{B}^T = \\begin{pmatrix}\n",
    "a_1, a_2, a_3\n",
    "\\end{pmatrix} \\cdot\n",
    "\\begin{pmatrix}\n",
    "b_1 \\\\\n",
    "b_2 \\\\\n",
    "b_3\n",
    "\\end{pmatrix} = a_1 \\cdot b_1 + a_2 \\cdot b_2 + a_3 \\cdot b_3\n",
    "$$\n",
    "\n",
    "相乘的结构为(1,3) x (3,1) = (1,1)，最终得到一个标量。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b78b7d-7018-4ead-8eea-3a5b003a59b2",
   "metadata": {},
   "source": [
    "在NLP的世界当中，我们所拿到的词向量数据或时间序列数据一定是具有多个样本的。我们需要求解**样本与样本两两之间的相关性**，综合该相关性分数，我们才能够计算出一个样本对于整个序列的重要性。在这里需要注意的是，在NLP的领域中，样本与样本之间的相关性计算、即向量的之间的相关性计算会受到向量顺序的影响。**这是说，以一个单词为核心来计算相关性，和以另一个单词为核心来计算相关性，会得出不同的相关程度**。举例说明：\n",
    "\n",
    "假设我们有这样一个句子：**我爱小猫咪。**\n",
    "\n",
    "> - 如果以\"我\"字作为核心词，计算“我”与该句子中其他词语的相关性，那么\"爱\"和\"小猫咪\"在这个上下文中都非常重要。\"爱\"告诉我们\"我\"对\"小猫咪\"的感情是什么，而\"小猫咪\"是\"我\"的感情对象。这个时候，\"爱\"和\"小猫咪\"与\"我\"这个词的相关性就很大。\n",
    "\n",
    "> - 但是，如果我们以\"小猫咪\"作为核心词，计算“小猫咪”与该剧自中其他词语的相关性，那么\"我\"的重要性就没有那么大了。因为不论是谁爱小猫咪，都不会改变\"小猫咪\"本身。这个时候，\"我\"对\"小猫咪\"这个词的上下文重要性就相对较小。\n",
    "\n",
    "当我们考虑更长的上下文时，这个特点会变得更加显著：\n",
    "\n",
    "> - 我爱小猫咪，但妈妈并不喜欢小猫咪。\n",
    "\n",
    "此时对猫咪这个词来说，谁喜欢它就非常重要。\n",
    "\n",
    "> - 我爱小猫咪，小猫咪非常柔软。\n",
    "\n",
    "此时对猫咪这个词来说，到底是谁喜欢它就不是那么重要了，关键是它因为柔软的属性而受人喜爱。\n",
    "\n",
    "因此，假设数据中存在A和B两个样本，则我们必须计算AB、AA、BA、BB四组相关性才可以。在每次计算相关性时，作为核心词的那个词被认为是在“询问”（Question），而作为非核心的词的那个词被认为是在“应答”（Key），AB之间的相关性就是A询问、B应答的结果，AA之间的相关性就是A向自己询问、A自己应答的结果。\n",
    "\n",
    "这个过程可以通过矩阵的乘法来完成。假设现在我们的向量中有2个样本（A与B），每个样本被编码为了拥有4个特征的词向量。如下所示，如果我们要计算A、B两个向量之间的相关性，只需要让特征矩阵与其转置矩阵做乘法就可以了——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e41e0f7-d265-48b6-8ba6-ecbbbb51e72a",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce9d240-c861-451c-906b-b6815d981755",
   "metadata": {},
   "source": [
    "该乘法规律可以推广到任意维度的数据上，因此面对任意的数据，我们只需要让该数据与自身的转置矩阵相乘，就可以自然得到样本与样本之间的相关性构成的相关性矩阵了。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/06_.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b27875-074d-4aa3-a01f-4c403e1098a7",
   "metadata": {},
   "source": [
    "当然，在实际计算相关性的时候，我们一般不会直接使用原始特征矩阵并让它与转置矩阵相乘，**因为我们渴望得到的是语义的相关性，而非单纯数字上的相关性**。因此在NLP中使用注意力机制的时候，**我们往往会先在原始特征矩阵的基础上乘以一个解读语义的$w$参数矩阵，以生成用于询问的矩阵Q、用于应答的矩阵K以及其他可能有用的矩阵**。\n",
    "\n",
    "在实际进行运算时，$w$是神经网络的参数，是由迭代获得的，因此$w$会依据损失函数的需求不断对原始特征矩阵进行语义解读，而我们实际的相关性计算是在矩阵Q和K之间运行的。使用Q和K求解出相关性分数的过程，就是自注意力机制的核心过程。\n",
    "\n",
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b9acd1-d216-4385-9930-47a76f3b6dd6",
   "metadata": {},
   "source": [
    "- **Transformer中的自注意力机制**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebf4cf3-f7a9-49b2-bd43-fa4bc5ce0a4a",
   "metadata": {},
   "source": [
    "现在我们知道注意力机制是如何运行的了，在Transformer当中我们具体是如何使用自注意力机制为样本增加权重的呢？来看下面的流程。\n",
    "\n",
    "**Step1：通过词向量得到QK矩阵**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9fab64-b32c-44d7-9a50-da0805c0575d",
   "metadata": {},
   "source": [
    "首先，transformer当中计算的相关性被称之为是**注意力分数**，该注意力分数是在原始的注意力机制上修改后而获得的全新计算方式，其具体计算公式如下——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89054cb-6fa7-43ff-ab74-1c262aec5e08",
   "metadata": {},
   "source": [
    "$$Attention(Q,K,V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79275bc4-fd44-49be-9eb8-9641fe633b11",
   "metadata": {},
   "source": [
    "在这个公式中，首先我们要先将原始特征矩阵转化为Q和K，然后令Q乘以K的转置，以获得最基础的相关性分数。同时，我们计算出权重之后，还需要将权重乘在样本上，以构成“上下文的复合表示”，因此我们还需要在原始特征矩阵基础上转化处矩阵V，用于表示原始特征所携带的信息值。假设现在我们有4个单词，每个单词被编码成了6列的词向量，那计算Q、K、V的过程如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4591b7a4-4e0e-4c42-87ee-1a97f6954473",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2b57b9-ecde-4c20-b97b-7fab967186b1",
   "metadata": {},
   "source": [
    "其中的$W_Q$与$W_K$的结构都为（6,3），事实上我们值需要保证这两个参数矩阵能够与$X$相乘即可（即这两个参数矩阵的行数与X被编码的列数相同即可），在现代大部分的应用当中，一般$W_Q$与$W_K$都是正方形的结构。\n",
    "\n",
    "**Step2：计算$QK$相似度，得到相关性矩阵**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0b8135-1426-4534-88ad-34182403f042",
   "metadata": {},
   "source": [
    "接下来我们让Q和K的转置相乘，计算出相关性矩阵。\n",
    "\n",
    "$$Attention(Q,K,V) = softmax(\\frac{QK^{T}}{\\sqrt{d_k}})V$$\n",
    "\n",
    "$QK^{T}$的过程中，点积是相乘后相加的计算流程，因此词向量的维度越高、点积中相加的项也就会越多，因此点积就会越大。此时，词向量的维度对于相关性分数是有影响的，在两个序列的实际相关程度一致的情况下，词向量的特征维度高更可能诞生巨大的相关性分数，因此对相关性分数需要进行标准化。在这里，Transformer为相关性矩阵设置了除以$\\sqrt{d_k}$的标准化流程，$d_k$就是特征的维度，以上面的假设为例，$d_k$=6。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32c4e92-2bc2-46a8-a47a-fe9345aeed96",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/Live/NLP%26LLMs/2-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e677a541-7b31-4355-89f2-46a46a7f392d",
   "metadata": {},
   "source": [
    "**Step3：softmax函数归一化**\n",
    "\n",
    "将每个单词之间的相关性向量转换成[0,1]之间的概率分布。例如，对AB两个样本我们会求解出AA、AB、BB、BA四个相关性，经过softmax函数的转化，可以让AA+AB的总和为1，可以让BB+BA的总和为1。这个操作可以令一个样本的相关性总和为1，从而将相关性分数转化成性质上更接近“权重”的[0,1]之间的比例。这样做也可以控制相关性分数整体的大小，避免产生数字过大的问题。\n",
    "\n",
    "经过Softmax归一化之后的分数，就是注意力机制求解出的**权重**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20287472-a299-44a2-92d0-3a1a5a50c40e",
   "metadata": {},
   "source": [
    "**Step4：对样本进行加权求和，建立样本与样本之间的关系**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a81cf-7d23-4481-9a7e-275d2327aed5",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e32837-f0d9-4094-b938-ebd57a3f9b68",
   "metadata": {},
   "source": [
    "现在我们已经获得了softmax之后的分数矩阵，同时我们还有代表原始特征矩阵值的V矩阵——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7966d731-6e83-447d-8e21-eb9cb8bdd817",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{score} = \\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix},\n",
    "\\quad\n",
    "\\mathbf{V} = \\begin{pmatrix}\n",
    "v_{11} & v_{12} & v_{13} \\\\\n",
    "v_{21} & v_{22} & v_{23}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "二者相乘的结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4231d2-1af1-4e1c-80a8-4fcb65489715",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{Attention} = \\begin{pmatrix}\n",
    "a_{11} & a_{12} \\\\\n",
    "a_{21} & a_{22}\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "v_{11} & v_{12} & v_{13} \\\\\n",
    "v_{21} & v_{22} & v_{23}\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "(a_{11}v_{11} + a_{12}v_{21}) & (a_{11}v_{12} + a_{12}v_{22}) & (a_{11}v_{13} + a_{12}v_{23}) \\\\\n",
    "(a_{21}v_{11} + a_{22}v_{21}) & (a_{21}v_{12} + a_{22}v_{22}) & (a_{21}v_{13} + a_{22}v_{23})\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f70f41-6666-42df-8446-7fd3202bbb6a",
   "metadata": {},
   "source": [
    "观察最终得出的结果，式子$a_{11}v_{11} + a_{12}v_{21}$不正是$v_{11}$和$v_{21}$的加权求和结果吗？$v_{11}$和v_{21}正对应着原始特征矩阵当中的第一个样本的第一个特征、以及第二个样本的第一个特征，这两个v之间加权求和所建立的关联，正是两个样本之间、两个时间步之间所建立的关联。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba8f279-efaf-479d-b0b1-c44108e35fe1",
   "metadata": {},
   "source": [
    "- **Multi-Head Attention 多头注意力机制**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9ab39d-e88e-4118-9110-16908d09a61b",
   "metadata": {},
   "source": [
    "Multi-Head Attention 就是在self-attention的基础上，对于输入的embedding矩阵，self-attention只使用了一组$W^Q,W^K,W^V$ 来进行变换得到Query，Keys，Values。而Multi-Head Attention使用多组$W^Q,W^K,W^V$  得到多组Query，Keys，Values，然后每组分别计算得到一个Z矩阵，最后将得到的多个Z矩阵进行拼接。Transformer原论文里面是使用了8组不同的$W^Q,W^K,W^V$  。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d8cc2f-16ef-4d50-9953-c11c1c3f1d32",
   "metadata": {},
   "source": [
    "![](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c841ff2-008b-4bde-aca2-caf563699989",
   "metadata": {},
   "source": [
    "![](https://data-science-blog.com/wp-content/uploads/2022/01/mha_img_original.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c706ea-07c9-426f-ba34-57114fa8e823",
   "metadata": {},
   "source": [
    "以上就是Transformer当中的自注意力层，Transformer就是在这一根本结构的基础上建立了样本与样本之间的链接。在此结构基础上，Transformer丰富了众多的细节来构成一个完整的架构。让我们现在就来看看Transformer的整体结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41349832-d5ee-41ca-9856-cbaf7fdab086",
   "metadata": {},
   "source": [
    "## 2 Transformer的基本结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4306b7-3723-471f-9d16-f5e6a0a76f91",
   "metadata": {},
   "source": [
    "让我们一起来看看Transformer算法都由哪些元素组成，以下是来自论文《All you need is Attention》的架构图："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db1c453-7eed-4cd4-b0a4-60e74b320627",
   "metadata": {},
   "source": [
    "<center><img src=\"https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png\" alt=\"描述文字\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98a56ce-c212-4eed-bb42-19136b96a5f0",
   "metadata": {},
   "source": [
    "Transformer的总体架构主要由两大部分构成：编码器（Encoder）和解码器（Decoder）。在Transformer中，编码是解构自然语言、将自然语言转化为计算机能够理解的信息，并让计算机深度学习数据、理解数据的结构，而解码器是让将算法深度处理过的数据还原回“自然语言”的过程，因此在transformer中，编码器负责接收输入数据，而解码器负责输出最终的标签。\n",
    "\n",
    "编码器（Encoder）结构包括两个子层：一个是自注意力（Self-Attention）层，另一个是前馈（Feed-Forward）神经网络。输入会先经过自注意力层，这层的作用是帮助模型关注输入序列中不同位置的信息。然后，经过前馈神经网络层，这是一个简单的全连接神经网络。两个子层都有一个残差连接（Residual Connection）和层标准化（Layer Normalization）。\n",
    "\n",
    "解码器（Decoder）也是由多层的解码器层组成。每个解码器层有三个子层：第一个也是自注意力层（只不过是携带掩码的自注意力层），第二个是注意力层（Attention），第三个是前馈神经网络。自注意力层和前馈神经网络的结构与编码器中的相同。注意力层是用来关注编码器输出的。同样的，每个子层都有一个残差连接和层标准化。\n",
    "\n",
    "现在就让我们从解码器部分开始逐一解读transformer结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad3022c-e161-462f-bdfe-23b59d0fb43a",
   "metadata": {},
   "source": [
    "<center><img src=\"https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-1.png\" alt=\"描述文字\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3358bb14-46ce-4556-b974-5e8fd9160c6a",
   "metadata": {},
   "source": [
    "### 2.1 Encoder的输入层与编码技术"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4884c4a-5e1f-409e-8937-189aebbacb31",
   "metadata": {},
   "source": [
    "在Transformer中，embedding层位于encoder和decoder之前，主要负责进行语义编码。然而，由于Transformer模型放弃了“逐行对数据进行处理”的方式，而是一次性处理一整张表单，因此它不能直接像循环神经网络RNN那样在训练过程中就捕捉到单词与单词之间的位置信息，因此Transformer引入了位置编码（positional encoding）技术来补充语义词嵌入。位置编码被加到词嵌入上，这样模型就可以同时知道一个词的语义和它在句子中的位置。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e9d14a-4d3b-4c9f-9e95-c91d11d0aa6e",
   "metadata": {},
   "source": [
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1aca67e-4fcd-4173-8791-209376d5426f",
   "metadata": {},
   "source": [
    "位置编码使用了一种特殊的函数，这个函数会为序列中的每个位置生成一个向量。对于一个特定的位置，这个函数生成的向量在所有维度上的值都是不同的。这保证了每个位置的编码都是唯一的，而且不同位置的编码能够保持一定的相对关系。**在transformer的位置编码中，我们需要对每个词的每个特征值给与位置编码**。\n",
    "\n",
    "在Transformer模型中，词嵌入和位置编码被相加，然后输入到模型的第一层。这样，Transformer就可以同时处理词语的语义和它在句子中的位置信息。这也是Transformer模型在处理序列数据，特别是自然语言处理任务中表现出色的一个重要原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90bc80a-3174-4983-9432-3ae8749ea4b2",
   "metadata": {},
   "source": [
    "在原论文中采用的是正弦、余弦位置编码公式：\n",
    "\n",
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-3.png)\n",
    "\n",
    "* pos是词语原始的位置编号。\n",
    "* d_model是embedding的维度，也就是被编码后的特征数量。\n",
    "* i是具体特征的索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f681df-9eec-40ae-a1b8-da04d337529b",
   "metadata": {},
   "source": [
    "对于多维的词向量，其偶数维度采用了sin函数来编码，奇数维度采用了cos函数来编码。\n",
    "\n",
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265b088-d45b-40a5-a624-764915bd8b2d",
   "metadata": {},
   "source": [
    "* 对于单词“i”（位置0）\n",
    "  * $PE_{(0,0)}=sin(0)=0$\n",
    "  * $PE_{(0,1)}=cos(0)=1$\n",
    "  * $PE_{(0,2)}=sin(0)=0$\n",
    "  * $PE_{(0,3)}=cos(0)=1$\n",
    "    所以单词i对应的位置编码为[0,1,0,1]\n",
    "* 对于单词“love”（位置1）\n",
    "  * $PE_{(1,0)}=sin(1/10000^{\\frac{2*0}{4}})=sin(1)\\approx 0.0174524064$\n",
    "  * $PE_{(1,1)}=cos(1/10000^{\\frac{2*0}{4}})=cos(1)\\approx 0.999847695$\n",
    "  * $PE_{(1,2)}=sin(1/10000^{\\frac{2*1}{4}})=sin(0.01)\\approx 0.000174532924$\n",
    "  * $PE_{(1,3)}=cos(1/10000^{\\frac{2*1}{4}})=cos(0.01)\\approx 0.999999985$\n",
    "  \n",
    "    所以单词love对应的位置编码约为[0.0174524064, 0.999847695, 0.0001745329240, 0.999999985]\n",
    "    \n",
    "* 对于单词“coding”（位置2）\n",
    "  * $PE_{(2,0)}=sin(2/10000^{\\frac{2*0}{4}})=sin(2)$\n",
    "  * $PE_{(2,1)}=cos(2/10000^{\\frac{2*0}{4}})=cos(2)$\n",
    "  * $PE_{(2,2)}=sin(2/10000^{\\frac{2*1}{4}})=sin(0.02)$\n",
    "  * $PE_{(2,3)}=cos(2/10000^{\\frac{2*1}{4}})=cos(0.02)$\n",
    "\n",
    "明白了具体的计算过程后我们来思考一下为什么使用正余弦位置编码呢？\n",
    "\n",
    "* 首先最重要的是其**泛化性**。在模型训练过程中，我们可能使用的都是序列长度小于20的数据，但是当实际应用中遇到一个序列长度为50的数据，**正弦和余弦函数的周期性**意味着，即使模型在训练时未见过某个位置，它仍然可以生成一个合理的位置编码。它可用泛化到不同长度的序列。\n",
    "* **不增加额外的训练参数**。当我们在一个已经很大的模型（如 GPT-3 或 BERT）上添加位置信息时，我们不希望增加太多的参数，因为这会增加训练成本和过拟合的风险。正弦和余弦位置编码不增加任何训练参数。\n",
    "* **相对位置信息带来的平滑变化**\n",
    "  在很多语言中，动词和宾语之间的相对位置十分重要。比如“写 代码”“写 作文”……尽管这些词在长句子中的绝对位置可能会变化，但它们之间的相对位置是关键的。\n",
    "  正弦和余弦位置编码的平滑变化确保了相邻位置的编码是相似的，而远离的位置的编码是不同的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a000479d-aba9-47af-81ee-a9afeb828588",
   "metadata": {},
   "source": [
    "### 2.2 Encoder结构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c357feb7-f752-47dc-8698-cd369d694d64",
   "metadata": {},
   "source": [
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25545e2-14cb-4b01-a1e2-9394b0b2f45b",
   "metadata": {},
   "source": [
    "- **残差连接**\n",
    "\n",
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-15.png)\n",
    "\n",
    "我们经过多头注意力机制得到$z_1$后并没有直接传入前馈神经网络，而是经过了一个**Add & Normalize**，这是为什么呢？\n",
    "\n",
    "我们看**Add**，加了一个什么呢？加了一个x自身。\n",
    "从一个残差块开始：\n",
    "\n",
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-16.png)\n",
    "\n",
    "上面的残差块来自何凯明在2015年提出的残差网络（ResNet）https://arxiv.org/abs/1512.03385。\n",
    "作者的灵感来源于如果只把浅层的输出做恒等映射（即F(X)=0）输入到深层，这样网络加深也并不会出现网络退化。所以，他在网络中加入了“短路”机制，并且这样不但解决了梯度消失问题，同时也提高了计算效率，可以从数学角度分析一下为什么会有这样的效果（非严格证明）。\n",
    "\n",
    "设 $X_i,X_{i+1}$, 为网络中某一较浅层残差块的输入输出（$X_{i+1}$同时为下一残差块的输入）， $X_I$为某一深层残差块的输出（可以把一个残差块理解为网络的一层），残差函数记为$F()$，记relu函数($r(x)=max(0,x)$)为$r()$（假设每一残差块的输入输出都为正），由此可得：\n",
    "\n",
    "$$X_{i+1}=r(X_i+F(X_i,W_i)) \\\\ =X_i+F(X_i,W_i)$$\n",
    "\n",
    "$$X_{i+2}=r(X_{i+1}+F(X_{i+1},W_{i+1}))  \\\\ =X_i+F(X_i,W_i)+F(X_{i+1},W_{i+1})$$\n",
    "$$...  ...$$ 如此递归推导可以得到\n",
    "$$X_I=X_i+\\sum_{n=i}^{I-1}F(X_n,W_n)$$\n",
    "\n",
    "在更新梯度的时候：\n",
    "\n",
    "$$\\frac{\\partial Loss}{\\partial X_{i}}=\\frac{\\partial Loss}{\\partial X_{I}}\\frac{\\partial X_{I}}{\\partial X_{i}} \\\\\n",
    "=\\frac{\\partial Loss}{\\partial X_{I}}\\frac{\\partial(X_i+\\sum_{n=i}^{I-1}F(X_n,W_n))}{\\partial X_{i}} \\\\\n",
    "=\\frac{\\partial Loss}{\\partial X_{I}}(1+\\frac{\\sum_{n=i}^{I-1}F(X_n,W_n)}{\\partial X_{i}})$$\n",
    "\n",
    "从结果可以看出，因为有“1”的存在，高层的梯度可以直接传递到低层，有效防止了梯度消失的情况。与此同时，残差网络在更新梯度时把一些乘法转变为了加法，同时也提高了计算效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf2e55-21ec-4378-801d-c0fe9df427e5",
   "metadata": {},
   "source": [
    "- **Layer Normalization**\n",
    "\n",
    "然后我们来看一下**Normalize**。Transformer的Normalize使用了2016年Jimmy Lei Ba等人的的论文《Layer Normalization》https://arxiv.org/abs/1607.06450v1\n",
    "\n",
    "为什么要进行Normalize呢？\n",
    "在神经网络进行训练之前，都需要对于输入数据进行Normalize归一化，目的有二：\n",
    "1.能够加快训练的速度。\n",
    "2.提高训练的稳定性。\n",
    "\n",
    "LN 是 Normalization（规范化）家族中的一员，由 Batch Normalization（BN）发展而来。基本上所有的规范化技术，都可以概括为如下的公式：\n",
    "\n",
    "$h_i = f(a_i) \\\\\n",
    "{h_i}^{'}=f(\\frac{g_i}{\\sigma_i}(a_i-u_i)+b_i)$\n",
    "\n",
    "对于隐层中某个节点的输出为对激活值$a_i$ 进行非线性变换$f()$ 后的 $h_i$\n",
    "先使用均值$u_i$和方差 $\\sigma_i$对$a_i$ 进行**分布调整**。\n",
    "如果以正态分布为例，就是把“高瘦”（红色）和“矮胖”（蓝紫色）的都调整回正常体型（绿色），把偏离x=0的（紫色）拉回中间来。\n",
    "\n",
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-28.png)\n",
    "\n",
    "* 这样可以将每一次迭代的数据调整为相同分布，消除极端值，提升训练稳定性。\n",
    "* 同时“平移”操作，可以让激活值落入$f()$的梯度敏感区间即梯度更新幅度变大，模型训练加快。\n",
    "\n",
    "然而，在梯度敏感区内，隐层的输出接近于“线性”，模型表达能力会大幅度下降。引入 gain 因子$g_i$ 和 bias 因子 $b_i$，为规范化后的分布再加入一点“个性”。\n",
    "\n",
    "注： $g_i$和$b_i$作为**模型参数训练得到**，$u_i$和 $\\sigma_i$在**限定的数据范围内统计得到**。\n",
    "\n",
    "**为什么使用Layer Normalization（LN）而不使用Batch Normalization（BN）呢？**\n",
    "\n",
    "BN 和 LN 的差别就在$u_i$和 $\\sigma_i$这里，前者在某一个 Batch 内统计某特定神经元节点的输出分布（跨样本），后者在某一次迭代更新中统计同一层内的所有神经元节点的输出分布（同一样本下）。\n",
    "![Alt text](https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-29.png)\n",
    "\n",
    "最初BN 是为 CNN 任务提出的，需要较大的 BatchSize 来保证统计量的可靠性，并在训练阶段记录全局的$u$和 $\\sigma$供预测任务使用。而LN是独立于batch大小的，它只对单个输入样本的所有特征进行规范化。\n",
    "\n",
    "* NLP任务中经常会处理长度不同的句子，使用LN时可以不考虑其它样本的长度。\n",
    "* 在某些情况下，当可用的内存有限或者为了加速训练而使用更小的batch时，BN因为batch数量不足而受到了限制。\n",
    "* 在某些NLP任务和解码设置中，模型可能会一个接一个地处理序列中的元素，而不是一次处理整个batch。这样BN就不是很适用了。\n",
    "* 在Transformer模型中有很深的层次和自注意机制。通过对每一层的输入进行规范化，可以防止值的爆炸或消失，从而帮助模型更快地收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ca0770-6e81-4e91-9ece-e2dde35680eb",
   "metadata": {},
   "source": [
    "- **Feed-Forward Networks**\n",
    "\n",
    "在每个子层中，Multi-Head Attention层都接了一个FFN层。\n",
    "\n",
    "$FFN(x)=max(0,xW_1+b_1)W_2+b_2$\n",
    "\n",
    "其中$W_1,W_2$是权重，$b_1,b_2$是偏置。\n",
    "\n",
    "这里的全连接层是一个两层的神经网络，先线性变换，然后ReLU非线性，再线性变换。\n",
    "\n",
    "* **增加非线性性**： Transformer的Multi-Head Attention允许模型在不同位置之间建立关系，但它本身是线性的。FFN对每个位置（或词）的处理是独立的，通过其隐藏层和非线性激活函数（如上述公式的ReLU或GELU）为模型引入了必要的非线性性，这增强了模型的表示能力。\n",
    "* **局部到全局的表示能力**：自注意力机制允许模型在整个输入序列上获得全局信息。相对地，FFN为模型提供了局部的、位置特定的变换能力。这两种机制的结合使模型能够同时学习局部特征和长距离的依赖关系。\n",
    "* **模型的灵活多样性** ：FFN模块提供了更多的参数，可以用于训练过程中来捕捉效果更好的更复杂的数据模式。把Transformer看作拼积木，由不同模块构成，自注意力机制模块处理输入之间的关系，而FFN模块处理单一输入的非线性变换。这种模块化的设计使得Transformer更加灵活和强大。\n",
    "\n",
    "总的来说，在Transformer里FFN和Attention相辅相成，包括长距离的依赖的同时也包括了局部的复杂模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b6dcc-861a-4102-900b-f70de77a1daf",
   "metadata": {},
   "source": [
    "# 【未完待续 - 公开课明日继续】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24196edd-0294-418e-90c3-a806c9664267",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
